## Claims made -- Are they actually true?
### AGI is 25-50 years out
I think it will be less time.

### AGI will take a few hundred years.
I think it will take less time.

### The internet drove forward AI development because of easier access to data.
This is probably true.

### A self-driving car will have to make ethical decisions.
I don't believe this.
- There will rarely ever be an *ethical* dillema on the road.
- The programmer will code in these kinds of decisions

### Making machines more intelligent than us will eventually be a reality.
I believe it.

### A machine with general intelligence will have consciousness.
I mean, that's kind of ambiguous without a clear definition of consciousness.  (Or is it supposed to *be* a definition?)

### Thinking and feeling will come together.
What do they mean by feeling?  Emotions?  I think this is unlikely.  Feelings are our way of approximating our utility function, but there are much more efficient ways to do so.

### You can look at AIs and see what makes them tick.
Um... No.  That's the whole difficulty with the AI alignment.

### AI is just finding a new way to do slavery.
Well, kind of, but not really.  It's much broader than that, and doesn't carry the main downside of slavery:  that the slaves don't like being enslaved.

### We're co-evovling with technology.
No, we aren't.  Evolution takes many generations.  We are just learning to use more technology.

### It's always going to be cheaper to get a product out there than a *safe* product out there.
Yeah, duh.

### The first person to develop strong AI  will rule the world.
Yes!  Why are governments not spending WAY more on AI research??

### There is an "event horizon" of intelligence.
I.e. the singularity can/will occur.
I'm less certain on this, but think it might be true.

## Assertion significantly less true in 2020 than in 2022
### Our present AI systems are very *very* narrow at what they do.
This is false.  Look at Gato, for example.

## Discussion Questions
-   Find one claim made in the movie, and try to investigate it further. What did you find?
Claim:  Whoever gets first to market with an AI will rule the world.
Question this makes me ask:  Why aren't governments funding AI like nuclear weapons?  Or ar they?
I'm investigating **how much governments are spending on AI research, and where that money is going**.

Okay--The goverment spends about 37 billion on all science research.  It also spent ~1.7 billion in 2020 on AI research.  This jumped up to 6 billion in 2021, and is probably even higher now.  So, yes, they actually are funding AI research like nuclear weapons, and continuing to spend more on it.  Most of the money is going into millitary applications.

-   What did you find most compelling about the movie?
It wasn't very compelling at all.  There were way too many dark side tactics.  Most compelling was probably the Space Odyssey quotes:  They were fun to watch but also showed how a slightly mis-aligned AI can create disastrous outcomes, a convincing argument for the dangers of AI.

-   What did you disagree with the most in the movie?
It's a toss-up between "AIs fighting in wars instead of humans is bad" and "AI slavery is bad."

-   What point do you wish had been made?
AI can do a tremendous amount of good as well.

-   Which of the possible threats from AI that was mentioned seems most important to you? What might convince you to change your mind?
The most important threat mentioned to me was that small mis-alignments in AIs can lead to disastrous outcomes.  What might convince me to change my mind is evidence that AIs learn what human values are through "osmosis" on their training data.  E.g., if a study came out that most LLMs pass >90% of ethics questions posed to them, I'd feel much more comfortable about this problem, since then we can hardcode in the AI asking itself whether what it's doing is ethical before taking actions.