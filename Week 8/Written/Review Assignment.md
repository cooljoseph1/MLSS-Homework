---
title:  Monitoring
author: Joseph Camacho
date: August 14, 2022
geometry: margin=1in
---
### 1
Even if an honest power-seeking AI tells us its plans, it doesn't mean we will be able to stop it.  For example, it may not start out seeking power, but eventually becomes smart enough that (1) it realizes gaining power is a good idea and (2) humans are unable to stop it.  In fact, a smart AI would almost surely decide to not seek power until it is too powerful to stop!

(What would you do if you were put in a cell and told that any escape attempts or dishonesty would be punished severely, and your captors can predict the future better than you?  I personally would precommit to not seeking to escape until I've had the ability to escape for a long enough time that their future-predicting wouldn't be able stop me.  In a similar vein, a smart AI should precommit to not seeking power until any monitoring systems humans have, including being able to tell if it plans to seek power soon, are unable to stop it.)

### 2
What's worse than the AI saying "yes" and then trying to kill us is it saying "no" and then still trying to kill us.  In the first case, at least, we have more warning to stop it.  Honesty helps us by giving us that warning.

### 3
Just because humans will cooperate easily with each other (which is a rather dubious notion) tells us nothing about whether AIs will cooperate easily with humans.  AIs will still want many of the same things we do (control over other people, popularity), and these desires may come into conflict.

### 4
There is no chain of evidence or reason connecting the if ("we can get multiple agent systems beneficial and safe") with the then clause ("single AI agents will be safe too").  I can't argue against a claim that doesn't exist, except to say that it is lacking substance.

### 5
