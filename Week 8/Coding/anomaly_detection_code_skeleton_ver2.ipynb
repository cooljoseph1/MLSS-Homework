{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection v3.0"
      ],
      "metadata": {
        "id": "RcZg-ysGbLym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note:\n",
        "It is recommended to run the code using a GPU. To do this, go to Runtime > Change runtime type > Hardware Accelerator > select GPU.\n",
        "\n",
        "It may be the case that a GPU is not available, in which case, use a default CPU (\"None\" hardware accelerator). The code will still work without a GPU, but may run much slower.\n",
        "\n",
        "### Contributers:\n",
        "Jason Ding"
      ],
      "metadata": {
        "id": "rVDP9Kt-EoJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites\n",
        "\n",
        "In the following cells, we are installing and importing the necessary libaries and downloading the classification model."
      ],
      "metadata": {
        "id": "xTF95S98RyLT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qo_2N4BQ3vdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d7c9aa-416b-46b7-bfbf-4abc97ad8d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-14 06:33:52--  https://github.com/hendrycks/outlier-exposure/raw/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hendrycks/outlier-exposure/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt [following]\n",
            "--2022-08-14 06:33:52--  https://raw.githubusercontent.com/hendrycks/outlier-exposure/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9037421 (8.6M) [application/octet-stream]\n",
            "Saving to: ‘cifar10_wrn_baseline_epoch_99.pt.2’\n",
            "\n",
            "cifar10_wrn_baselin 100%[===================>]   8.62M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-08-14 06:33:54 (298 MB/s) - ‘cifar10_wrn_baseline_epoch_99.pt.2’ saved [9037421/9037421]\n",
            "\n",
            "--2022-08-14 06:33:54--  https://raw.githubusercontent.com/hendrycks/pre-training/master/robustness/adversarial/models/wrn_with_pen.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3913 (3.8K) [text/plain]\n",
            "Saving to: ‘wrn_with_pen.py.2’\n",
            "\n",
            "wrn_with_pen.py.2   100%[===================>]   3.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-08-14 06:33:54 (65.8 MB/s) - ‘wrn_with_pen.py.2’ saved [3913/3913]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision==0.12.0 in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.21.6)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/hendrycks/outlier-exposure/raw/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt\n",
        "!wget https://raw.githubusercontent.com/hendrycks/pre-training/master/robustness/adversarial/models/wrn_with_pen.py\n",
        "!pip3 install torchvision==0.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pRRReMOZyRLW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import sklearn.metrics as sk\n",
        "from wrn_with_pen import WideResNet\n",
        "\n",
        "prefetch = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and Data Loading\n",
        "\n",
        "In the following cells, we are loading the CIFAR-10 model. Additionally, we are retrieving CIFAR-10 and out-of-distribution datasets."
      ],
      "metadata": {
        "id": "qu6Z4vj6RJJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M4dZ_8Es3cFw"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 model\n",
        "\n",
        "net = WideResNet(depth=40, num_classes=10, widen_factor=2, dropRate=0.3)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    net.load_state_dict(torch.load('cifar10_wrn_baseline_epoch_99.pt'))\n",
        "    net.eval()\n",
        "    net.cuda()\n",
        "else:\n",
        "    net.load_state_dict(torch.load('cifar10_wrn_baseline_epoch_99.pt', map_location=torch.device('cpu')))\n",
        "    net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6n7lrVCvCDN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde6cd50-fd04-4d4b-b5d0-1942a9daa6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Using downloaded and verified file: data/test_32x32.mat\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# /////////////// Loading Datasets ///////////////\n",
        "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "\n",
        "# /////////////// CIFAR-10 ///////////////\n",
        "\n",
        "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "cifar_10_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "cifar10_data = cifar_10_data\n",
        "cifar10_loader = torch.utils.data.DataLoader(cifar10_data, batch_size=200, shuffle=False,\n",
        "                                          num_workers=prefetch, pin_memory=True)\n",
        "ood_num_examples = len(cifar10_data) // 5\n",
        "\n",
        "# /////////////// Rademacher Noise ///////////////\n",
        "\n",
        "dummy_targets = torch.ones(ood_num_examples)\n",
        "ood_data = torch.from_numpy(np.random.binomial(\n",
        "    n=1, p=0.5, size=(ood_num_examples, 3, 32, 32)).astype(np.float32)) * 2 - 1\n",
        "rademacher_ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
        "rademacher_ood_loader = torch.utils.data.DataLoader(rademacher_ood_data, batch_size=200, shuffle=True)\n",
        "\n",
        "# /////////////// SVHN ///////////////\n",
        "\n",
        "data_transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "svhn_ood_data = torchvision.datasets.SVHN(root = \"data\", \n",
        "                          split=\"test\",\n",
        "                          transform = data_transform, \n",
        "                          download = True)\n",
        "\n",
        "svhn_ood_loader = torch.utils.data.DataLoader(svhn_ood_data, batch_size=200, shuffle=True,\n",
        "                                         num_workers=prefetch, pin_memory=True)\n",
        "\n",
        "# /////////////// DTD ///////////////\n",
        "\n",
        "# data_transform = transforms.Compose([transforms.Resize(32), transforms.CenterCrop(32),\n",
        "#                                      transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "# dtd_ood_data = torchvision.datasets.DTD(root = \"data\", \n",
        "#                           split=\"test\",\n",
        "#                           transform = data_transform, \n",
        "#                           download = True)\n",
        "\n",
        "# dtd_ood_loader = torch.utils.data.DataLoader(dtd_ood_data, batch_size=200, shuffle=True,\n",
        "#                                          num_workers=prefetch, pin_memory=True)\n",
        "\n",
        "# /////////////// CIFAR-100 ///////////////\n",
        "\n",
        "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "cifar100_ood_data = datasets.CIFAR100(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "cifar100_ood_loader = torch.utils.data.DataLoader(cifar100_ood_data, batch_size=200, shuffle=True,\n",
        "                                         num_workers=prefetch, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Functions\n",
        "\n",
        "The following cells define functions that we will use in the rest of the code. You do not need to do anything here."
      ],
      "metadata": {
        "id": "OmMQ8yfaSLWm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bIeyVeYM6yVL"
      },
      "outputs": [],
      "source": [
        "concat = lambda x: np.concatenate(x, axis=0)\n",
        "to_np = lambda x: x.data.cpu().numpy()\n",
        "\n",
        "'''\n",
        "Calculates the anomaly scores for a portion of the given dataset. \n",
        "If a GPU is not available, will run on a smaller fraction of the\n",
        "dataset, so that calculations will be faster.\n",
        "\n",
        "loader: A DataLoader that contains the loaded data of a dataset\n",
        "anomaly_score_calculator: A function that takes in the output \n",
        "                          logit of a batch of data and/or the \n",
        "                          penultimate.\n",
        "model_net: The classifier model.\n",
        "usePenultimate: True if anomaly_score_calculator needs the \n",
        "                penultimate as a parameter. False otherwise.\n",
        "'''\n",
        "def get_ood_scores(loader, anomaly_score_calculator, model_net, use_penultimate = False):\n",
        "    _score = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            if torch.cuda.is_available():\n",
        "                fraction = 200\n",
        "            else:\n",
        "                fraction = 1000\n",
        "            if batch_idx >= ood_num_examples // fraction:\n",
        "                break\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data = data.cuda()\n",
        "\n",
        "            output = model_net(data)\n",
        "\n",
        "            if use_penultimate:\n",
        "                score = anomaly_score_calculator(output[0], output[1])\n",
        "            else:\n",
        "                score = anomaly_score_calculator(output[0])\n",
        "            _score.append(score)\n",
        "\n",
        "    return concat(_score).copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /////////////// Printing Results ///////////////\n",
        "all_anomaly_results = {}\n",
        "\n",
        "'''\n",
        "Returns and prints out the AUROC score of a dataset.\n",
        "\n",
        "ood_loader: A DataLoader that contains the loaded data of a dataset\n",
        "anomaly_score_calculator: A function that takes in the output \n",
        "                          logit of a batch of data and/or the \n",
        "                          penultimate.\n",
        "model_net: The classifier model.\n",
        "usePenultimate: True if anomaly_score_calculator needs the \n",
        "                penultimate as a parameter. False otherwise.\n",
        "'''\n",
        "def get_and_print_results(ood_loader, anomaly_score_calculator, model_net, use_penultimate):\n",
        "    out_score = get_ood_scores(ood_loader, anomaly_score_calculator, model_net, use_penultimate = use_penultimate)\n",
        "    auroc = get_auroc(out_score, in_score)\n",
        "    print('AUROC: \\t\\t\\t{:.2f}'.format(100 * auroc) + \"%\")\n",
        "    return auroc\n",
        "\n",
        "'''\n",
        "Prints out the AUROC score of all the OOD datasets. The results \n",
        "will be appended to global variable all_anomaly_results, which \n",
        "is used later for display purposes.\n",
        "\n",
        "anomaly_score_calculator: A function that takes in the output \n",
        "                          logit of a batch of data and/or the \n",
        "                          penultimate.\n",
        "anomaly_score_name: The name of the anomaly score method.\n",
        "model_net: The classifier model.\n",
        "model_name: The name of the classifier model.\n",
        "usePenultimate: True if anomaly_score_calculator needs the \n",
        "                penultimate as a parameter. False otherwise.\n",
        "'''\n",
        "def print_all_results(anomaly_score_calculator, anomaly_score_name, model_net, model_name = \"default_model\", use_penultimate = False):\n",
        "    global in_score, all_anomaly_results\n",
        "    in_score = get_ood_scores(cifar10_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results = []\n",
        "\n",
        "    print('Rademacher Noise Detection')\n",
        "    auroc = get_and_print_results(rademacher_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results.append(auroc)\n",
        "\n",
        "    print('\\nSVHN Detection')\n",
        "    auroc = get_and_print_results(svhn_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results.append(auroc)\n",
        "\n",
        "    # print('\\nDTD Detection')\n",
        "    # auroc = get_and_print_results(dtd_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    # results.append(auroc)\n",
        "\n",
        "    print('\\nCIFAR-100 Detection')\n",
        "    auroc = get_and_print_results(cifar100_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results.append(auroc)\n",
        "\n",
        "    average = sum(results) / len(results)\n",
        "    results.append(average)\n",
        "\n",
        "    if not model_name in all_anomaly_results:\n",
        "        all_anomaly_results[model_name] = {}\n",
        "    all_anomaly_results[model_name][anomaly_score_name] = results"
      ],
      "metadata": {
        "id": "uPaE51LaTJ88"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement AUROC Score\n",
        "Fill in the get_auroc score. We will use this function in order to calculate the AUROC score of an out-of-distribution dataset.\n",
        "\n",
        "It may be helpful to use the sklearn.metrics.roc_auc_score() function. Both _pos and _neg should be used."
      ],
      "metadata": {
        "id": "epCT4PxHGJ3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Calculates the AUROC score of a OOD dataset.\n",
        "\n",
        "_pos: an array of anomoly scores of the OOD dataset\n",
        "_neg: an array of anomoly scores of images in the CIFAR-10 dataset\n",
        "return: The AUROC score the data in decimal form\n",
        "'''\n",
        "def get_auroc(_pos, _neg):\n",
        "    ############################################################################\n",
        "    # TODO:  Calculate the AUROC score.                                        #\n",
        "    ############################################################################\n",
        "    print(type(_pos), len(_pos))\n",
        "    pos_augmented = [(-x, 1) for x in _pos]\n",
        "    neg_augmented = [(-x, 0) for x in _neg]\n",
        "    combined = sorted(pos_augmented + neg_augmented)\n",
        "    signs = [x[1] for x in combined]\n",
        "    total = 0\n",
        "    running_total = 0\n",
        "    for s in signs:\n",
        "        if s == 0:\n",
        "            total += running_total\n",
        "        else:\n",
        "            running_total += 1\n",
        "    auroc_score = total / len(_pos) / len(_neg)\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    return auroc_score"
      ],
      "metadata": {
        "id": "FFLwtHgRFWK5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Anomaly Score Calculators\n",
        "Fill in the folllowing functions which calculate the anomaly score given the model's output for a batch of data (the output will contain one logit per image in the batch).\n",
        "\n",
        "The following equations show how the logits should be transformed in order to get the anomaly score.\n",
        "\n",
        "Max Logit Equation: <br>\n",
        "$\\text{Score}=-\\text{max} l_k$\n",
        "\n",
        "Max Softmax Equation: <br>\n",
        "$\\text{Score}=-\\text{max} p(y=k|x)$\n",
        "\n",
        "Cross Entropy Anomaly Equation: <br>\n",
        "$\\text{Score} = \\bar{l}-\\text{log}∑_{c=1}^{\\text{num_classes}}e^{l_c}$"
      ],
      "metadata": {
        "id": "Ot-KkzW1K55s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lajsgp2BXnc2"
      },
      "outputs": [],
      "source": [
        "# /////////////// Anomaly Score Calculators ///////////////\n",
        "\n",
        "'''\n",
        "Calculates the max logit anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "'''\n",
        "def max_logit_anomaly_score(output):\n",
        "    ############################################################################\n",
        "    # TODO: Calculate the max logit anomaly score (returning a numpy array).   #\n",
        "    ############################################################################\n",
        "    result = -torch.amax(output, dim=1)\n",
        "    result = result.cpu()\n",
        "    return result\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "\n",
        "'''\n",
        "Calculates the max softmax anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "'''\n",
        "def max_softmax_anomaly_score(output):\n",
        "    ############################################################################\n",
        "    # TODO: Calculate the max softmax anomaly score (returning a numpy array). #\n",
        "    ############################################################################\n",
        "    adjusted = output - output.amax(dim=1, keepdim=True)\n",
        "    exp = torch.exp(adjusted)\n",
        "    y = exp / torch.sum(exp, dim=1, keepdim=True)\n",
        "    result = -torch.amax(y, dim=1)\n",
        "    result = result.cpu()\n",
        "    return result\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "\n",
        "'''\n",
        "Calculates the cross entropy anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "'''\n",
        "def cross_entropy_anomaly_score(output):\n",
        "    ############################################################################\n",
        "    # TODO: Calculate the cross entropy anomaly score (returning a numpy array).#\n",
        "    ############################################################################\n",
        "    means = output.mean(dim=1)\n",
        "    logsumexp = torch.logsumexp(output, dim=1)\n",
        "    result = means - logsumexp\n",
        "    result = result.cpu()\n",
        "    return result\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print AUROC Results\n",
        "\n",
        "Run the following cells in order to see how well each of the anomaly score calculators do on the OOD datasets."
      ],
      "metadata": {
        "id": "tmqvyCn-X3RE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wz78Dx8PM05O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24dce71-1cf1-4e08-e131-833e0945d58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Logit AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t69.95%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t91.20%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t86.64%\n"
          ]
        }
      ],
      "source": [
        "print(\"======= Max Logit AUROC Scores =======\")\n",
        "print_all_results(max_logit_anomaly_score, \"Max Logit\", net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jKL87S6dMyFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8c3d23-d53b-4b8b-c0a1-149db24022bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Softmax AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t80.31%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t92.27%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t87.94%\n"
          ]
        }
      ],
      "source": [
        "print(\"======= Max Softmax AUROC Scores =======\")\n",
        "print_all_results(max_softmax_anomaly_score, \"Max Softmax\", net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Yosj6ofYV9bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db1d2c4-4725-4a79-860a-6cde10afbf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Cross Entropy AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t69.99%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t91.49%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t86.66%\n"
          ]
        }
      ],
      "source": [
        "print(\"======= Cross Entropy AUROC Scores =======\")\n",
        "print_all_results(cross_entropy_anomaly_score, \"Cross Entropy\", net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement ViM\n",
        "\n",
        "We will now implement virtual-logit matching (ViM). ViM works by doing the following. \n",
        "\n",
        "ViM first calculates the **principal space** ($P$) using the features obtained by passing the training data (CIFAR-10) through to just before the final fully-connected layer. Let us use the 12 most significant principal components for our principal space. HINT: You may find it helpful to use np.linalg.svd to find the principal components (or you can use eignvalues and eigenvectors).\n",
        "\n",
        "Then, ViM calculates **alpha** ($\\alpha$) by projecting the training data's features onto the space orthogonal to the prinicpal space calculated before ($P^\\perp$). This is known as the **residual** ($x^{P^\\perp}$). HINT: the space orthogonal to the principle space can be found simply by taking the remaining principal components (i.e. all PCs apart from the first 12).\n",
        "\n",
        "Alpha is then calculated by the following equation:\n",
        "> $\\alpha := \\frac{∑^k_{i=1}\\text{max}_{j=1,...,C}\\{l^i_j\\}}{∑^K_{i=1}\\| x_i^{P\\perp} \\|}$\n",
        "\n",
        "In other words, alpha is the sum of each logit's maximum value, divided by the sum of the norms of each feature's residuals.\n",
        "\n",
        "Your calculated alpha in this part should be around 16.16.\n"
      ],
      "metadata": {
        "id": "nq3lnDCbZ6qK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "erJ538Vl1qYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fb44bc-0faf-4095-98e2-e2389112a498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing alpha...\n",
            "alpha = 16.16054344177246\n"
          ]
        }
      ],
      "source": [
        "# You may find the following functions useful.\n",
        "from numpy.linalg import pinv, norm\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "_score = []\n",
        "to_np = lambda x: x.data.cpu().numpy()\n",
        "concat = lambda x: np.concatenate(x, axis=0)\n",
        "\n",
        "# Extraction fully connected layer's weights and biases\n",
        "w, b = net.fc.weight.cpu().detach().numpy(), net.fc.bias.cpu().detach().numpy()\n",
        "# Origin of a new coordinate system of feature space to remove bias\n",
        "u = -np.matmul(pinv(w), b)\n",
        "\n",
        "'''\n",
        "Calculates and returns the alpha values given the training data the model used.\n",
        "\n",
        "training_data_loader: A DataLoader that contains the loaded data of a \n",
        "                      training dataset.\n",
        "model_net: The classifier model.\n",
        "verbose: If true, will print out the alpha value.\n",
        "return: Returns the alpha value.\n",
        "'''\n",
        "def compute_ViM_principal_space_and_alpha(training_data_loader, model_net, verbose = False):\n",
        "    result = []\n",
        "\n",
        "    # Getting the first batch of the training data to calculate principal space and alpha\n",
        "    training_data, target = next(iter(training_data_loader))\n",
        "    if torch.cuda.is_available():\n",
        "        training_data = training_data.cuda()\n",
        "\n",
        "    result = model_net(training_data)\n",
        "    logit = result[0] # Logits (values before softmax)\n",
        "    penultimate = result[1] # Features/Penultimate (values before fully connected layer)\n",
        "\n",
        "    logit_id_train = logit.cpu().detach().numpy().squeeze()\n",
        "    feature_id_train = penultimate.cpu().detach().numpy().squeeze()\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO:  Calculate the space orthogonal to the pricipal space and then     #\n",
        "    # compute alpha.                                                           #\n",
        "    ############################################################################\n",
        "    if verbose:\n",
        "        print('Computing alpha...')\n",
        "\n",
        "    penultimate = penultimate.detach().cpu()\n",
        "    penultimate = penultimate - u\n",
        "    penultimate = penultimate.cuda()\n",
        "    U, S, V = torch.svd(penultimate)\n",
        "    principal_space = V[:, 12:]\n",
        "    penultimate_projected = penultimate @ principal_space @ principal_space.T\n",
        "    \n",
        "    alpha_num = logit.amax(dim=1).sum()\n",
        "    alpha_denom = torch.linalg.norm(penultimate_projected, dim=1).sum()\n",
        "    alpha = alpha_num / alpha_denom\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    if verbose:\n",
        "        print(f'alpha = {alpha}')\n",
        "\n",
        "    return principal_space, alpha\n",
        "\n",
        "principal_space, alpha = compute_ViM_principal_space_and_alpha(cifar10_loader, net, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement ViM Anomaly Score Calculator\n",
        "\n",
        "Now, implement the ViM anomaly score calculator. \n",
        "\n",
        "First, we want to project our penultimate/feature values onto the principal space which we found before, which is called the residual. We multiply the norm of this residual by alpha to get what we call the **virtual logit score**. \n",
        "\n",
        "Next, we get what we call the **energy score** by taking LogSumExp of the logits. \n",
        "\n",
        "Finally, the outputted **anomaly score** is calculated by subtracting the virtual logit by the energy score.\n",
        "\n",
        "$\\text{vlogit}= \\alpha \\| {x^{P^\\perp}} \\|$\\\n",
        "$\\text{energy} = \\text{ln}\\sum_{i=1}^C e^{l_i}$\\\n",
        "$\\text{anomaly_score} = \\text{vlogit} - \\text{energy}$"
      ],
      "metadata": {
        "id": "rmP5powXLnM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jzikAO1QYRSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed480d9-4639-4ab1-d727-062898fdfc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= ViM_anomaly_score_calculator =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t99.97%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t95.42%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t82.96%\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Calculates the ViM anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "penultimate: The model's penultimate (feature values) for a batch of data.\n",
        "'''\n",
        "def ViM_anomaly_score_calculator(output, penultimate):\n",
        "    logit_id_val = output.cpu().detach().numpy().squeeze()\n",
        "    feature_id_val = penultimate.cpu().detach().numpy().squeeze()\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO:  Calculate the anomaly score.                                      #\n",
        "    ############################################################################\n",
        "    x_projected = penultimate @ principal_space @ principal_space.T\n",
        "    vlogit = alpha * torch.norm(x_projected, dim=1)\n",
        "    energy = torch.logsumexp(output, dim=1)\n",
        "    anomaly_score = vlogit - energy\n",
        "    anomaly_score = anomaly_score.cpu()\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    \n",
        "    return anomaly_score\n",
        "\n",
        "print(\"======= ViM_anomaly_score_calculator =======\")\n",
        "w, b = net.fc.weight.cpu().detach().numpy(), net.fc.bias.cpu().detach().numpy()\n",
        "u = -np.matmul(pinv(w), b)\n",
        "principal_space, alpha = compute_ViM_principal_space_and_alpha(cifar10_loader, net) # Making sure you have the correct ViM values before calculating the score \n",
        "print_all_results(ViM_anomaly_score_calculator, \"ViM\", net, use_penultimate = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Anomaly Score Results\n",
        "Run the following cell to see how the different anomaly score calculators compare to each other for the OOD datasets. You should see that ViM is superior to other anomaly scores in all of the datasets."
      ],
      "metadata": {
        "id": "jeNAD556O5gN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HIJ-zCyvJYp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e535aea4-a571-444d-b3c8-a01b5a5c933e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            default_model |  Rademacher  |     SVHN     |  CIFAR-100   |   Average   \n",
            "=====================================================================================\n",
            "                Max Logit |    69.95%    |    91.20%    |    86.64%    |    82.60%   \n",
            "              Max Softmax |    80.31%    |    92.27%    |   *87.94%    |    86.84%   \n",
            "            Cross Entropy |    69.99%    |    91.49%    |    86.66%    |    82.71%   \n",
            "                      ViM |   *99.97%    |   *95.42%    |    82.96%    |   *92.78%   \n",
            "\n",
            "\n",
            "* highlights the maximum AUROC Score for an OOD Dataset\n"
          ]
        }
      ],
      "source": [
        "# ///////////////// Compare Results /////////////////\n",
        "\n",
        "def get_results_max(model_name = \"normal\"):\n",
        "    all_anomaly_results[model_name][\"max\"] = [0,0,0,0,0]\n",
        "    for key in all_anomaly_results[model_name].keys():\n",
        "        if (key != \"max\"):\n",
        "            index = 0\n",
        "            for score in all_anomaly_results[model_name][key]:\n",
        "                all_anomaly_results[model_name][\"max\"][index] = \\\n",
        "                    max(score, all_anomaly_results[model_name][\"max\"][index])\n",
        "                index += 1\n",
        "\n",
        "\n",
        "def compare_all_results():\n",
        "    for model_name in all_anomaly_results:\n",
        "        to_be_printed = \" \" * (25 - len(model_name)) + model_name\n",
        "        dataset_names = [\"Rademacher\", \"SVHN\", \"CIFAR-100\", \"Average\"]\n",
        "        for name in dataset_names:\n",
        "            to_be_printed += \" | \" + \" \"*(6-math.ceil(len(name)/2)) + \\\n",
        "                                name + \" \"*(6-math.floor(len(name)/2))\n",
        "\n",
        "        print(to_be_printed)\n",
        "        print(\"=\" * (25 + len(dataset_names) * 15))\n",
        "\n",
        "        get_results_max(model_name = model_name)\n",
        "        for key in all_anomaly_results[model_name].keys():\n",
        "            if (key != \"max\"):\n",
        "                to_be_printed = \" \"*(25-len(key)) + key\n",
        "                index = 0\n",
        "                for result in all_anomaly_results[model_name][key]:\n",
        "                    if (all_anomaly_results[model_name][\"max\"][index] == result):\n",
        "                        result = \"*\" + '{:.2f}'.format(round(result * 100, 2)) + \"%\"\n",
        "                    else:\n",
        "                        result = '{:.2f}'.format(round(result * 100, 2)) + \"%\"\n",
        "                    to_be_printed += \" | \" + \" \"*(6-math.ceil(len(result)/2)) + \\\n",
        "                                        result + \" \"*(6-math.floor(len(result)/2))\n",
        "                    index += 1\n",
        "                print(to_be_printed)\n",
        "        print()\n",
        "\n",
        "    print(\"\\n* highlights the maximum AUROC Score for an OOD Dataset\")\n",
        "\n",
        "compare_all_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Now we will see if models trained using data augmentation methods for robustness help in OOD detection. \n",
        "\n",
        "We will load a CIFAR-10 model that used PixMix data augmentation during training, and see how it fares compared to the default model that did not use data augmentation."
      ],
      "metadata": {
        "id": "lOftK8KC8fSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1mjIfbb3mfXXvAZ1sBnjotFr5yYFmLi68 # Downloading PixMix model\n",
        "!gdown 1skZT6yplO-Sv4M8Ksgzx14cTY3H-HgOa # Downlaoding wideresnet class"
      ],
      "metadata": {
        "id": "ANy7vzim9PbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b89ba2-8d5f-4fd9-b83f-b62771fd6d96"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mjIfbb3mfXXvAZ1sBnjotFr5yYFmLi68\n",
            "To: /content/checkpoint.pth.tar\n",
            "100% 71.8M/71.8M [00:01<00:00, 60.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1skZT6yplO-Sv4M8Ksgzx14cTY3H-HgOa\n",
            "To: /content/wideresnet_with_pen.py\n",
            "100% 4.03k/4.03k [00:00<00:00, 6.42MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wideresnet_with_pen import WideResNet as WideResNet2"
      ],
      "metadata": {
        "id": "wx8QTsLj9QAy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the PixMix model\n",
        "\n",
        "pixmix_net = WideResNet2(depth=40, num_classes=10, widen_factor=4, drop_rate=0.3)\n",
        "pixmix_net = torch.nn.DataParallel(pixmix_net)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    checkpoint = torch.load('checkpoint.pth.tar')\n",
        "    pixmix_net.load_state_dict(checkpoint['state_dict'])\n",
        "    net.eval()\n",
        "    net.cuda()\n",
        "else:\n",
        "    checkpoint = torch.load('checkpoint.pth.tar', map_location=torch.device('cpu'))\n",
        "    pixmix_net.load_state_dict(checkpoint['state_dict'])\n",
        "    net.eval()"
      ],
      "metadata": {
        "id": "TQiyBmip9eYr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PixMix Model Testing\n",
        "\n",
        "Let us now test the PixMix model with the same anomaly score calculators we coded before."
      ],
      "metadata": {
        "id": "XRje54Ip-w3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= Max Logit AUROC Scores =======\")\n",
        "print_all_results(max_logit_anomaly_score, \"Max Logit\", pixmix_net, model_name = \"pixmix_trained_model\")"
      ],
      "metadata": {
        "id": "W_MVibEl9mgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6324f7c7-bf1e-4343-8533-f981e049a86b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Logit AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t94.62%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t92.67%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t87.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= Max Softmax Probability AUROC Scores =======\")\n",
        "print_all_results(max_softmax_anomaly_score, \"Max Softmax Probability\", pixmix_net, model_name = \"pixmix_trained_model\")"
      ],
      "metadata": {
        "id": "q3Nx8ieU9npx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8be271-ecc8-449e-e0c7-ceeaf54696f7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Softmax Probability AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t92.45%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t90.52%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t87.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= Cross Entropy AUROC Scores =======\")\n",
        "print_all_results(cross_entropy_anomaly_score, \"Cross Entropy\", pixmix_net, model_name = \"pixmix_trained_model\")"
      ],
      "metadata": {
        "id": "umUyonHj9oGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faff9eb2-5569-4732-f103-2ead0db2c5e9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Cross Entropy AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t94.61%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t92.29%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t86.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= ViM_anomaly_score_calculator =======\")\n",
        "w, b = pixmix_net.module.fc.weight.cpu().detach().numpy(), pixmix_net.module.fc.bias.cpu().detach().numpy()\n",
        "u = -np.matmul(pinv(w), b)\n",
        "principal_space, alpha = compute_ViM_principal_space_and_alpha(cifar10_loader, pixmix_net)\n",
        "print_all_results(ViM_anomaly_score_calculator, \"ViM\", pixmix_net, model_name = \"pixmix_trained_model\", use_penultimate = True)"
      ],
      "metadata": {
        "id": "mPyLCKXK9rtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149e0857-61ba-46c7-9f4d-34ce070ad0f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= ViM_anomaly_score_calculator =======\n",
            "Rademacher Noise Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t89.35%\n",
            "\n",
            "SVHN Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t92.37%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "<class 'numpy.ndarray'> 2000\n",
            "AUROC: \t\t\t90.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare No-data-augmentation Model vs PixMix Model\n",
        "\n",
        "Let us now compare how the default model compared to the PixMix model by running the following cell. \n",
        "\n",
        "You should see that the PixMix model successfully helps us in OOD detection, and has a higher AUROC score."
      ],
      "metadata": {
        "id": "jt-XKgcq-8G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_all_results()"
      ],
      "metadata": {
        "id": "xBa5Gz0K9sTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034c555d-ee0d-4d82-9863-eae67759c4df"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            default_model |  Rademacher  |     SVHN     |  CIFAR-100   |   Average   \n",
            "=====================================================================================\n",
            "                Max Logit |    69.95%    |    91.20%    |    86.64%    |    82.60%   \n",
            "              Max Softmax |    80.31%    |    92.27%    |   *87.94%    |    86.84%   \n",
            "            Cross Entropy |    69.99%    |    91.49%    |    86.66%    |    82.71%   \n",
            "                      ViM |   *99.97%    |   *95.42%    |    82.96%    |   *92.78%   \n",
            "\n",
            "     pixmix_trained_model |  Rademacher  |     SVHN     |  CIFAR-100   |   Average   \n",
            "=====================================================================================\n",
            "                Max Logit |   *94.62%    |   *92.67%    |    87.68%    |   *91.66%   \n",
            "  Max Softmax Probability |    92.45%    |    90.52%    |    87.63%    |    90.20%   \n",
            "            Cross Entropy |    94.61%    |    92.29%    |    86.84%    |    91.25%   \n",
            "                      ViM |    89.35%    |    92.37%    |   *90.97%    |    90.90%   \n",
            "\n",
            "\n",
            "* highlights the maximum AUROC Score for an OOD Dataset\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "anomaly_detection_code_skeleton_ver2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}