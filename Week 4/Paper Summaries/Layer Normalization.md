Layer normalization consists of normalizing each layer in the neural network so that the expected value of the neurons are $0$ and their variance is $1$.  Note that this differs from batch normalization because batch normalization takes the expected value and variance for a *single* neuron over all the training data (or an estimate done by batches) whereas layer normalization takes the expected value and variance over all the neruons in a single layer in a single instance of the training data.