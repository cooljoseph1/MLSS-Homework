# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
I'm a bit confused about this paper.  Didn't word2vec come out five years beforehand, and do basically the same thing (guess words based on the surrounding words)?  What's their innovation?  Is it that they used attention, a larger model, and more surrounding words?  I feel like they really should have cited the word2vec paper....