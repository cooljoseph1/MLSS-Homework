## Article: [Is Power-Seeking AI an Existential Risk?](https://www.josephcarlsmith.com/_files/ugd/5f37c1_5333aa0b7ff7461abc208b25bfc7df87.pdf)


### Question
Carlsmith writes,

> Nuclear contamination is hard to clean up, and to stop from spreading. But it isn’t trying to not get cleaned up, or trying to spread—and especially not with greater intelligence than the humans trying to contain it. But the power-seeking agents just described would be trying, in sophisticated ways, to undermine our efforts to stop them.

This appears to be an essential difference between many hazards humans encounter and the potential hazard posed by power-seeking AI. However, some hazards we currently face do have some of the properties described above. Can you think of any? Do you think our efforts to deal with them could be instructive for AI risk?

### Answer
Yes:  Superviruses, large asteroids hitting the Earth, solar flares.
Not really.  I mostly agree with his hypothesis that the main risk from AI will be because the AI is purposely trying, in sophisticated ways, to undermine our efforts of stopping them.

### Question
Carlsmith is writing particularly about “APS” agents: agents that have advanced capabilities, can plan, and are strategically aware. Do you think he enumerates the important considerations? Are there any that he missed?

### Answer
I think he misses the factors "has long term goals" (i.e. goals that provide it a utility far into the past and/or extend for long periods of time) and "desiring wholeheartedly to achieve its goals".  If a Just because an agent has goals does not imply it will strive its best to achieve them, or even care about keeping them the same!

Also:  He claims that "goal-content integrity" is a convergent instrumental goal, but I disagree.  An AI should only care that it's goals are changed if it cares that it's goals are changed.  It's circular logic!