## Article: [Is Power-Seeking AI an Existential Risk?](https://www.josephcarlsmith.com/_files/ugd/5f37c1_5333aa0b7ff7461abc208b25bfc7df87.pdf)


### Question
Carlsmith writes,

> Nuclear contamination is hard to clean up, and to stop from spreading. But it isn’t trying to not get cleaned up, or trying to spread—and especially not with greater intelligence than the humans trying to contain it. But the power-seeking agents just described would be trying, in sophisticated ways, to undermine our efforts to stop them.

This appears to be an essential difference between many hazards humans encounter and the potential hazard posed by power-seeking AI. However, some hazards we currently face do have some of the properties described above. Can you think of any? Do you think our efforts to deal with them could be instructive for AI risk?

### Answer
Yes:  Superviruses, large asteroids hitting the Earth, solar flares.
Not really.  I mostly agree with his hypothesis that the main risk from AI will be because the AI is purposely trying, in sophisticated ways, to undermine our efforts of stopping them.

### Question
Carlsmith is writing particularly about “APS” agents: agents that have advanced capabilities, can plan, and are strategically aware. Do you think he enumerates the important considerations? Are there any that he missed?

### Answer
I think he misses the factors "has long term goals" (i.e. goals that provide it a utility far into the past and/or extend for long periods of time) and "desiring wholeheartedly to achieve its goals".  If a Just because an agent has goals does not imply it will strive its best to achieve them, or even care about keeping them the same!

Also:  He claims that "goal-content integrity" is a convergent instrumental goal, but I disagree.  An AI should only care that it's goals are changed if it cares that it's goals are changed.  It's circular logic!

### Question
Carlsmith writes:
>Optimizing a system to perform some not-intuitively-agential task (for example, predicting strings of text) could, given sufficient cognitive sophistication, result in internal computation that makes and executes plans, in pursuit of objectives, on the basis of broad and informed models of the real world, even if the designers of the system do not expect this (they may even be unable to tell whether it has occurred).

Do you think this is true? Give a concrete example where something like this could occur. How likely is it?


### Answer
I think this point is a bit disingenuous.  That their example of "predicting strings of text" requires planning is obvious to anyone who thinks about it for a bit.  It's not really hidden that it's an "agential" task.  The only reason it might be confusing at first is that we typically think of an "agent" as someone with **independent, self-directed** goals, which is not exactly what Carlsmith means by "agent".  In essence, Carlsmith's use of the word "agent" is anthropomorphisizing AIs he really shouldn't be.

Other examples of unconventional "agents" include translators and the prophecy orbs in the Harry Potter universe.