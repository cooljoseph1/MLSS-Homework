## Article: [Is Power-Seeking AI an Existential Risk?](https://www.josephcarlsmith.com/_files/ugd/5f37c1_5333aa0b7ff7461abc208b25bfc7df87.pdf)


### Question
Carlsmith writes,

> Nuclear contamination is hard to clean up, and to stop from spreading. But it isn’t trying to not get cleaned up, or trying to spread—and especially not with greater intelligence than the humans trying to contain it. But the power-seeking agents just described would be trying, in sophisticated ways, to undermine our efforts to stop them.

This appears to be an essential difference between many hazards humans encounter and the potential hazard posed by power-seeking AI. However, some hazards we currently face do have some of the properties described above. Can you think of any? Do you think our efforts to deal with them could be instructive for AI risk?

### Answer
Yes:  Superviruses, large asteroids hitting the Earth, solar flares.
Not really.  I mostly agree with his hypothesis that the main risk from AI will be because the AI is purposely trying, in sophisticated ways, to undermine our efforts of stopping them.

### Question
Carlsmith is writing particularly about “APS” agents: agents that have advanced capabilities, can plan, and are strategically aware. Do you think he enumerates the important considerations? Are there any that he missed?

### Answer
I think he misses the factors "has long term goals" (i.e. goals that provide it a utility far into the past and/or extend for long periods of time) and "desiring wholeheartedly to achieve its goals".  If a Just because an agent has goals does not imply it will strive its best to achieve them, or even care about keeping them the same!

Also:  He claims that "goal-content integrity" is a convergent instrumental goal, but I disagree.  An AI should only care that it's goals are changed if it cares that it's goals are changed.  It's circular logic!

### Question
Carlsmith writes:
>Optimizing a system to perform some not-intuitively-agential task (for example, predicting strings of text) could, given sufficient cognitive sophistication, result in internal computation that makes and executes plans, in pursuit of objectives, on the basis of broad and informed models of the real world, even if the designers of the system do not expect this (they may even be unable to tell whether it has occurred).

Do you think this is true? Give a concrete example where something like this could occur. How likely is it?

### Answer
I think this point is a bit disingenuous.  That their example of "predicting strings of text" requires planning is obvious to anyone who thinks about it for a bit.  It's not really hidden that it's an "agential" task.  The only reason it might be confusing at first is that we typically think of an "agent" as someone with **independent, self-directed** goals, which is not exactly what Carlsmith means by "agent".  In essence, Carlsmith's use of the word "agent" is anthropomorphisizing AIs he really shouldn't be.

Other examples of unconventional "agents" include translators and any future forecaster that anticipates it's answers will be used.

### Question
Carlsmith discusses the idea of instrumental convergence, and it is also covered in the optional readings. Brainstorm some goals an AI might have that aren’t mentioned in any of the readings. Can you imagine ways that power could be helpful for achieving those goals?

### Answer
Rationality -- It makes the AI more accurate in its predictions and more efficient with its intelligence.

Curiosity (i.e. the desire to try new things) -- This greatly helps the AI learn more (resource acquisition).

### Question
Carlsmith mentions that there are problems with proxies. Think of a measurable value that might appear good for an AI to optimize. Now think of some ways that optimizing that proxy to the extreme could be problematic. 

### Answer
Proxy:  What fraction of humans vote that they think its actions are good?
Problems:  Kill every human except some small sub-population that is okay with the rest of humanity's death (e.g. if they are some indigenous tribe without contact with the outside world) and mostly approves of the actions they see.

Proxy:  What *number* of humans vote that they think its actions are good?
Problems:  Clone humans that like your actions, killing/disregarding everyone else.


### Question
Analyze the following statements. Focus on relevant considerations rather than just trying to decide if a statement is true or not.
- If AI does not actively seek power, then it cannot create an existential catastrophe.
- Deep learning cannot possibly be deceptive or power-seeking, because it’s just a mechanism with some matrix multiplies and nonlinear functions.
- Power-seeking is just an anthropomorphism. Non-human systems won’t seek power like humans do.
- We could solve this problem by just building altruistic systems that live in harmony with humans.

### Answer
- False.  Consider diseases--they don't actively seek power, but superbugs are x-risks.  AIs can similarly unintentionally cause existential catastrophe, but it's much less likely than purposely doing so.
-  Wait, what?  That's ridiculous.
- Sure, they won't seek power exactly like the kind of power humans seek, but that doesn't mean they won't seek power.
- Hahahahaha.  Like we did with animals, right?